{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(pair):\n",
    "    savedfile = os.path.join('data','CIFAR10_pair_{}_{}.npz'.format(pair[0], pair[1]))\n",
    "    if os.path.exists(savedfile):\n",
    "        npzfile = np.load(savedfile)\n",
    "        X_train, y_train, X_test, y_test = npzfile['X_train'], npzfile['y_train'], npzfile['X_test'], npzfile['y_test']\n",
    "        npzfile.close()\n",
    "    else:\n",
    "        print(\"There doesn't exist this file, please generate is firstly!!\")\n",
    "        \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data(('deer','truck'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Generate batch size data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index): # return tensor\n",
    "        img, target = self.images[index], self.labels[index]\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>define the network: input->100->100->100->output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(155, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 100)\n",
    "        self.fc4 = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "1.00000e-02 *\n",
      " 5.0982 -1.7782 -1.8291  ...   4.3955  6.6469  3.2374\n",
      "-4.1105 -0.3786 -2.0299  ...   4.0706  5.8239  2.1263\n",
      "-5.8567  5.6422 -7.4803  ...  -2.4854  2.3020 -4.6388\n",
      "          ...             ⋱             ...          \n",
      " 4.4779 -1.1620  4.8530  ...  -1.4632 -6.3117 -4.8190\n",
      " 6.4935 -0.9340  5.4549  ...  -1.9950 -5.6207 -7.9258\n",
      "-5.1423 -5.2672  3.9603  ...  -4.1129  3.6176 -3.3729\n",
      "[torch.FloatTensor of size 100x155]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-02 *\n",
      "  4.2008\n",
      "  1.0177\n",
      " -4.6503\n",
      "  6.2146\n",
      "  3.4745\n",
      "  6.8726\n",
      " -2.4897\n",
      "  6.1407\n",
      "  0.3371\n",
      " -0.3157\n",
      "  2.6664\n",
      "  7.0019\n",
      " -0.0039\n",
      "  0.9887\n",
      " -4.4269\n",
      " -0.9346\n",
      " -6.9708\n",
      "  5.6805\n",
      "  3.3760\n",
      "  4.8193\n",
      " -2.9886\n",
      "  5.9854\n",
      " -1.2185\n",
      "  1.3330\n",
      " -6.8400\n",
      " -6.1491\n",
      "  6.8941\n",
      "  7.7208\n",
      "  3.6163\n",
      " -6.8246\n",
      " -4.2586\n",
      " -5.5526\n",
      " -2.8910\n",
      "  5.5672\n",
      " -2.3639\n",
      " -2.2958\n",
      "  5.9755\n",
      " -6.6431\n",
      "  6.0871\n",
      " -1.6171\n",
      "  0.9280\n",
      " -0.4531\n",
      " -0.4017\n",
      "  7.3068\n",
      "  0.8126\n",
      " -2.0650\n",
      " -2.4020\n",
      " -2.7385\n",
      "  5.5543\n",
      "  0.7891\n",
      "  3.2362\n",
      "  6.6199\n",
      "  1.0803\n",
      "  6.9611\n",
      "  3.8270\n",
      " -5.7544\n",
      " -4.2418\n",
      " -2.7357\n",
      "  7.1185\n",
      " -0.3293\n",
      " -8.0056\n",
      " -0.6736\n",
      " -0.7554\n",
      " -4.3635\n",
      " -3.2548\n",
      " -4.7375\n",
      " -1.9440\n",
      "  4.3288\n",
      " -6.8293\n",
      "  5.1719\n",
      "  2.7203\n",
      "  7.5690\n",
      "  3.5282\n",
      "  6.0973\n",
      "  4.8574\n",
      " -5.5774\n",
      " -0.9825\n",
      "  0.7995\n",
      "  1.1324\n",
      "  2.6764\n",
      "  7.5468\n",
      "  1.8295\n",
      " -2.3184\n",
      "  4.3474\n",
      " -5.0857\n",
      " -3.9405\n",
      " -7.7291\n",
      " -3.6765\n",
      "  1.4265\n",
      " -1.1666\n",
      " -0.2811\n",
      " -1.4766\n",
      "  3.6445\n",
      "  0.4863\n",
      "  1.2484\n",
      " -7.9821\n",
      " -2.6843\n",
      "  2.6960\n",
      "  0.9905\n",
      "  2.1720\n",
      "[torch.FloatTensor of size 100]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-02 *\n",
      "-6.6053  1.6347  0.4199  ...  -4.5283  9.2007  9.7618\n",
      "-0.5015  8.8848 -4.9827  ...  -8.8421 -1.8884  5.1957\n",
      " 5.1070  5.5738  9.1463  ...   2.4356 -2.8460  5.8843\n",
      "          ...             ⋱             ...          \n",
      " 5.9397 -2.2960 -1.7245  ...   7.3963  4.2823  8.1716\n",
      "-4.0174 -8.5987 -5.2247  ...  -3.6512 -7.3618 -1.0490\n",
      " 3.2551  6.1308  0.6258  ...  -8.1915  7.2138 -2.9044\n",
      "[torch.FloatTensor of size 100x100]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-02 *\n",
      " -9.3963\n",
      " -0.0938\n",
      " -9.3407\n",
      "  7.8619\n",
      " -6.0223\n",
      " -9.0808\n",
      "  3.4407\n",
      " -0.3491\n",
      "  1.4646\n",
      " -3.4686\n",
      " -4.9272\n",
      " -9.2253\n",
      " -8.6987\n",
      " -8.5372\n",
      " -7.4310\n",
      "  3.4746\n",
      "  0.4805\n",
      "  4.1770\n",
      " -8.4686\n",
      "  3.6020\n",
      " -9.1484\n",
      " -5.9219\n",
      " -0.4306\n",
      "  5.2210\n",
      " -4.7087\n",
      "  7.0308\n",
      "  5.5139\n",
      "  3.3625\n",
      " -1.8773\n",
      " -9.6598\n",
      " -8.0257\n",
      " -5.7047\n",
      " -3.8569\n",
      " -4.4337\n",
      " -1.1438\n",
      " -2.0376\n",
      " -1.1443\n",
      "  9.4516\n",
      " -7.3979\n",
      " -7.9114\n",
      "  7.2952\n",
      "  0.8044\n",
      " -0.1857\n",
      "  6.2001\n",
      " -5.3471\n",
      " -9.3064\n",
      "  9.5259\n",
      " -2.7815\n",
      "  0.7798\n",
      " -1.5213\n",
      " -9.3590\n",
      " -8.1449\n",
      " -7.8304\n",
      " -9.4054\n",
      "  5.1597\n",
      "  9.6957\n",
      "  7.1848\n",
      "  6.2159\n",
      "  3.0204\n",
      "  0.2916\n",
      " -3.5127\n",
      "  5.1936\n",
      " -7.9074\n",
      " -4.3743\n",
      " -0.4866\n",
      " -1.6335\n",
      " -5.4922\n",
      " -3.5859\n",
      " -8.9905\n",
      " -3.4108\n",
      "  6.8489\n",
      " -3.2578\n",
      "  6.9011\n",
      " -9.8453\n",
      " -9.5361\n",
      " -9.1890\n",
      " -6.5871\n",
      " -3.0150\n",
      "  9.8948\n",
      " -6.6614\n",
      " -5.9729\n",
      "  0.3571\n",
      " -9.4032\n",
      " -2.3014\n",
      "  4.0232\n",
      "  0.7385\n",
      "  4.0856\n",
      " -1.5419\n",
      "  5.4352\n",
      " -3.0814\n",
      " -5.2331\n",
      " -3.3394\n",
      "  0.1121\n",
      " -4.3919\n",
      "  4.4675\n",
      " -6.6169\n",
      "  0.7315\n",
      " -0.3908\n",
      "  4.8397\n",
      "  2.0224\n",
      "[torch.FloatTensor of size 100]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-02 *\n",
      " 3.3438 -3.9235 -2.8233  ...  -1.4420 -1.4480 -3.3376\n",
      "-0.5232  3.7381  6.1542  ...  -0.3857  8.0613  6.7257\n",
      " 1.1340  0.5036  7.4888  ...   7.0146  2.3909  7.8562\n",
      "          ...             ⋱             ...          \n",
      " 9.0670 -8.7657 -9.7980  ...  -3.3696  5.2068  5.6474\n",
      " 5.3016  1.2527 -6.6869  ...  -9.6410  7.9250  0.1679\n",
      "-2.1812 -4.7560 -3.3543  ...  -9.6682 -3.3588  9.4669\n",
      "[torch.FloatTensor of size 100x100]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-02 *\n",
      " -3.7701\n",
      " -9.6899\n",
      " -5.0180\n",
      " -6.0170\n",
      " -3.3093\n",
      "  9.1350\n",
      " -7.2401\n",
      " -3.5202\n",
      "  3.4934\n",
      " -6.3814\n",
      " -7.2200\n",
      "  2.2698\n",
      "  7.8300\n",
      "  6.3681\n",
      "  9.5856\n",
      "  2.8159\n",
      " -7.9505\n",
      "  2.4943\n",
      "  8.4578\n",
      "  6.6503\n",
      "  7.8049\n",
      " -6.6396\n",
      "  3.3543\n",
      "  7.4704\n",
      "  4.6458\n",
      " -8.4741\n",
      " -6.9380\n",
      "  9.5046\n",
      "  3.7268\n",
      "  2.5576\n",
      "  0.5230\n",
      "  8.8919\n",
      " -4.8430\n",
      "  8.7839\n",
      " -5.2675\n",
      "  2.6247\n",
      "  7.2592\n",
      "  8.6773\n",
      "  2.4308\n",
      " -8.1070\n",
      "  0.3970\n",
      " -6.6421\n",
      " -2.5137\n",
      " -9.2424\n",
      "  5.2559\n",
      " -6.2293\n",
      " -5.3845\n",
      "  6.7377\n",
      "  6.4209\n",
      " -9.1450\n",
      " -6.1729\n",
      " -7.5840\n",
      "  8.5579\n",
      "  2.4786\n",
      "  4.3494\n",
      "  4.1898\n",
      "  8.4334\n",
      "  9.7039\n",
      "  1.7872\n",
      " -7.5543\n",
      "  8.4061\n",
      "  5.7555\n",
      " -4.9168\n",
      "  2.8443\n",
      " -7.5636\n",
      " -5.9885\n",
      " -7.3977\n",
      "  9.6563\n",
      "  5.4700\n",
      "  0.1290\n",
      "  8.7192\n",
      " -6.8218\n",
      " -7.8519\n",
      "  5.6456\n",
      "  8.0197\n",
      " -6.9193\n",
      " -7.9305\n",
      "  5.0868\n",
      " -9.9232\n",
      " -5.0290\n",
      " -7.6772\n",
      " -4.6325\n",
      "  1.0571\n",
      " -3.6440\n",
      "  3.2565\n",
      "  6.2704\n",
      " -9.1139\n",
      "  7.2045\n",
      "  5.5791\n",
      " -3.1058\n",
      " -2.5277\n",
      " -8.9033\n",
      " -5.5678\n",
      "  3.4641\n",
      " -9.2098\n",
      "  7.1427\n",
      " -3.0863\n",
      " -0.3535\n",
      "  1.5731\n",
      "  5.7199\n",
      "[torch.FloatTensor of size 100]\n",
      "\n",
      "Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "1.00000e-02 *\n",
      " -3.9380 -0.1303  0.7935 -9.7110  2.6557 -8.4874 -1.9342  5.3809 -4.1855  6.9193\n",
      "\n",
      "Columns 10 to 19 \n",
      "1.00000e-02 *\n",
      "  5.4057  6.9230 -4.4925 -0.3593 -2.5988 -7.2755 -5.8637 -9.8681 -9.8369  4.9745\n",
      "\n",
      "Columns 20 to 29 \n",
      "1.00000e-02 *\n",
      "  8.8586  7.7838  1.8481 -2.1853 -6.2841 -9.8364 -9.4401 -1.5220  5.7796  8.5479\n",
      "\n",
      "Columns 30 to 39 \n",
      "1.00000e-02 *\n",
      "  2.5943  3.1417  7.3467  0.5201  4.2974 -6.1871  4.6755  2.7890  5.8789 -7.0684\n",
      "\n",
      "Columns 40 to 49 \n",
      "1.00000e-02 *\n",
      " -5.2481  4.6551  1.2289  8.1469  9.3829  1.3925 -0.7760  8.3032 -5.9820 -6.4842\n",
      "\n",
      "Columns 50 to 59 \n",
      "1.00000e-02 *\n",
      " -7.2727  8.0185  5.5165  5.7691 -9.7115  4.2350 -4.1778  9.1279 -5.4198  3.4090\n",
      "\n",
      "Columns 60 to 69 \n",
      "1.00000e-02 *\n",
      "  9.7814  4.1399 -2.4643  5.9148 -8.7406 -9.9629  7.8318  4.4360  0.6925  3.8621\n",
      "\n",
      "Columns 70 to 79 \n",
      "1.00000e-02 *\n",
      "  9.5884  7.4477  9.1152  8.9476  1.9929 -3.6260 -9.3508  2.2641  4.6878 -4.5553\n",
      "\n",
      "Columns 80 to 89 \n",
      "1.00000e-02 *\n",
      "  3.7196  7.8650  8.1899 -4.2983 -4.2579  6.8665  7.3774 -8.2777 -2.6489 -7.9980\n",
      "\n",
      "Columns 90 to 99 \n",
      "1.00000e-02 *\n",
      "  9.5605  6.7151  8.3534  7.5472 -4.8757 -1.9405 -5.9627  7.0175  9.3983  0.0109\n",
      "[torch.FloatTensor of size 1x100]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-03 *\n",
      "  8.3268\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "for parameters in model.parameters():\n",
    "    print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type Variable[torch.LongTensor] but found type Variable[torch.FloatTensor] for argument #1 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f5b1d67e5e28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/avance/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type Variable[torch.LongTensor] but found type Variable[torch.FloatTensor] for argument #1 'target'"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "for step, (data, target) in enumerate(train_loader):\n",
    "    data = data.float()\n",
    "    target = data.float()\n",
    "    \n",
    "    data, target = Variable(data), Variable(target)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    output = model(data)\n",
    "    test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "    pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Adanet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 100\n",
    "\n",
    "class SimpleConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self,previous_nodes_layers=[],connection_weights=[],connection_bias=[],output_weights=[],output_bias=[],isAddLayer=False):\n",
    "        super(SimpleConvolutionalNetwork, self).__init__() \n",
    "        self.nodes_layers = previous_nodes_layers\n",
    "        self.connection_weight = connection_weights\n",
    "        self.connection_bias = connection_bias\n",
    "        self.output_weight = output_weights\n",
    "        self.output_bias = output_bias\n",
    "        self.connection_functions = []\n",
    "        self.output_functions = []\n",
    "        self.new_connection_index=0\n",
    "        index_connection = 0\n",
    "        if previous_nodes_layers == []:\n",
    "            self.nodes_layers.append(0)\n",
    "            fc = nn.Linear(155,B)\n",
    "            fo = nn.Linear(B,1)\n",
    "            self.connection_functions.append(fc)\n",
    "            self.output_functions.append(fo)\n",
    "        else:\n",
    "            if isAddLayer:\n",
    "                for i in range(max(previous_nodes_layers)+1):\n",
    "                    self.nodes_layers.append(i)\n",
    "                for i in range(len(self.nodes_layers)-(max(previous_nodes_layers)+1)):\n",
    "\n",
    "                    if self.nodes_layers[i] == 0:\n",
    "                        fc = nn.Linear(155,B)\n",
    "                        fc.weight = nn.Parameter(self.connection_weight[index_connection],requires_grad=False)\n",
    "                        fc.bias = nn.Parameter(self.connection_bias[index_connection],requires_grad=False)\n",
    "                        self.connection_functions.append(fc)\n",
    "                        index_connection+=1\n",
    "                        fo = nn.Linear(B,1)\n",
    "                        fo.weight = nn.Parameter(self.output_weight[i],requires_grad=False)\n",
    "                        fo.bias = nn.Parameter(self.output_bias[i],requires_grad=False)\n",
    "                        self.output_functions.append(fo)\n",
    "                    else:\n",
    "                        for j in range(i):\n",
    "                            if self.nodes_layers[j] == (self.nodes_layers[i]-1):\n",
    "                                fc = nn.Linear(B,B)\n",
    "                                fc.weight = nn.Parameter(self.connection_weight[index_connection],requires_grad=False)\n",
    "                                fc.bias = nn.Parameter(self.connection_bias[index_connection],requires_grad=False)\n",
    "                                self.connection_functions.append(fc)\n",
    "                                index_connection+=1\n",
    "                        fo = nn.Linear(B,1)\n",
    "                        fo.weight = nn.Parameter(self.output_weight[i],requires_grad=False)\n",
    "                        fo.bias = nn.Parameter(self.output_bias[i],requires_grad=False)\n",
    "                        self.output_functions.append(fo)\n",
    "                self.new_connection_index = index_connection\n",
    "                for i in range(max(previous_nodes_layers)+1):\n",
    "                    if i==0:\n",
    "                        fc = nn.Linear(155,B)\n",
    "                        self.connection_functions.append(fc)\n",
    "                        fo = nn.Linear(B,1)\n",
    "                        self.output_functions.append(fo)\n",
    "                    else:\n",
    "                        for j in range(len(self.nodes_layers)-(max(previous_nodes_layers)+1-j)):\n",
    "                            if self.nodes_layers[j] == (i-1):\n",
    "                                fc = nn.Linear(B,B)\n",
    "                                self.connection_functions.append(fc)\n",
    "                            fo = nn.Linear(B,1)\n",
    "                            self.output_functions.append(fo)\n",
    "\n",
    "            else:\n",
    "                for i in range(max(previous_nodes_layers)):\n",
    "                    self.nodes_layers.append(i)\n",
    "                    for i in range(len(self.nodes_layers)-(max(previous_nodes_layers))):\n",
    "                    #index_connection = 0\n",
    "                        if self.nodes_layers[i] == 0:\n",
    "                            fc = nn.Linear(155,B)\n",
    "                            fc.weight = nn.Parameter(self.connection_weight[index_connection],requires_grad=False)\n",
    "                            fc.bias = nn.Parameter(self.connection_bias[index_connection],requires_grad=False)\n",
    "                            self.connection_functions.append(fc)\n",
    "                            index_connection+=1\n",
    "                            fo = nn.Linear(B,1)\n",
    "                            fo.weight = nn.Parameter(self.output_weight[i],requires_grad=False)\n",
    "                            fo.bias = nn.Parameter(self.output_bias[i],requires_grad=False)\n",
    "                            self.output_functions.append(fo)\n",
    "                        else:\n",
    "                            for j in range(i):\n",
    "                                if self.nodes_layers[j] == (self.nodes_layers[i]-1):\n",
    "                                    fc = nn.Linear(B,B)\n",
    "                                    fc.weight = nn.Parameter(self.connection_weight[index_connection],requires_grad=False)\n",
    "                                    fc.bias = nn.Parameter(self.connection_bias[index_connection],requires_grad=False)\n",
    "                                    self.connection_functions.append(fc)\n",
    "                                    index_connection+=1\n",
    "                            \n",
    "                            fo = nn.Linear(B,1)\n",
    "                            fo.weight = nn.Parameter(self.output_weight[i],requires_grad=False)\n",
    "                            fo.bias = nn.Parameter(self.output_bias[i],requires_grad=False)\n",
    "                            self.output_functions.append(fo)\n",
    "                    self.new_connection_index = index_connection\n",
    "                for i in range(max(previous_nodes_layers)):\n",
    "                    if i==0:\n",
    "                        fc = nn.Linear(155,B)\n",
    "                        self.connection_functions.append(fc)\n",
    "                        fo = nn.Linear(B,1)\n",
    "                        self.output_functions.append(fo)\n",
    "                    else:\n",
    "                        for j in range(len(self.nodes_layers)-(max(previous_nodes_layers)-j)):\n",
    "                            if self.nodes_layers[j] == (i-1):\n",
    "                                fc = nn.Linear(B,B)\n",
    "                                self.connection_functions.append(fc)\n",
    "                            fo = nn.Linear(B,1)\n",
    "                            self.output_functions.append(fo)       \n",
    "\n",
    "    def forward(self, x):\n",
    "        index_connection=0\n",
    "        node_value=[]\n",
    "        output = 0\n",
    "        if len(self.nodes_layers)==1:\n",
    "            value = F.relu(self.connection_functions[index_connection](x))\n",
    "            output += self.output_functions[i](value)\n",
    "            return output\n",
    "        for i in range(max(self.nodes_layers)):\n",
    "            node_value.append([])\n",
    "        for i in range(len(self.nodes_layers)):\n",
    "            if (self.nodes_layers[i]==0):\n",
    "                value = F.relu(self.connection_functions[index_connection](x))\n",
    "                node_value[0].append(value)\n",
    "                index_connection+=1\n",
    "                output += self.output_functions[i](value)\n",
    "            else:\n",
    "                for j in range(i):\n",
    "                    num = 0\n",
    "                    if self.nodes_layers[j] == (self.nodes_layers[i]-1):\n",
    "                        value = np.zeros(B)\n",
    "                        value += F.relu(self.connection_functions[index_connection](node_value[self.nodes_layers[i]-1][num]))\n",
    "                        num+=1\n",
    "                        index_connection+=1\n",
    "                        node_value[self.nodes_layers[i]].append(value)\n",
    "                output += self.output_functions[i](value)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def retrun_parameters(self):\n",
    "        if previous_nodes_layers == []:\n",
    "            \n",
    "            self.connection_weight.append(self.connection_functions[0].weight)\n",
    "            self.connection_bias.append(self.connection_functions[0].bias)\n",
    "            self.output_weight.append(self.output_functions[0].weight)\n",
    "            self.output_bias.append(self.output_functions[0].bias)\n",
    "        else:\n",
    "            for i in (range(len(self.nodes_layers-len(previous_nodes_layers)))+len(previous_nodes_layers)-1):\n",
    "                self.output_weight.append(self.output_functions[i].weight)\n",
    "                self.output_bias.append(self.output_functions[i].bias)\n",
    "            length = len(self.connection_functions) - self.new_connection_index\n",
    "            for i in (range(length)+self.new_connection_index):\n",
    "                self.connection_weight.append(self.connection_functions[i].weight)\n",
    "                self.connection_bias.append(self.connection_functions[i].bias)\n",
    "        \n",
    "        return  self.nodes_layers, self.connection_weight, self.connection_bias, self.output_weight, \\\n",
    "                self.output_bias\n",
    "        \n",
    "    def return_update_parameters(self):\n",
    "        return self.output_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-1851137a4ea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleConvolutionalNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/avance/lib/python3.6/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdampening\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nesterov momentum requires a momentum and zero dampening\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/avance/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "model = SimpleConvolutionalNetwork([],[],[],[],[],False)\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
