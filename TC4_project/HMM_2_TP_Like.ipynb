{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# __Second-Order HMM for typos correction__\n",
    "_Author_ : Xudong ZHANG and Qixiang PENG  \n",
    "_Date_   : Mon Jan 9 23:53:48 CEST 2017  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import array, ones, zeros, multiply\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "UNK = \"<unk>\"  # token to map all out-of-vocabulary letters (OOVs)\n",
    "UNKid = 0      # index for UNK\n",
    "epsilon = 1e-100\n",
    "class HMM_2:\n",
    "    def __init__(self, state_list, observation_list,\n",
    "                 transition_proba=None,\n",
    "                 observation_proba=None,\n",
    "                 initial_state_proba=None, smoothing=0.01):\n",
    "        \"\"\"\n",
    "        Builds a Hidden 2 order Markov Model\n",
    "        * state_list is the list of state symbols [q_0...q_(N-1)]\n",
    "        * observation_list is the list of observation symbols [v_0...v_(M-1)]\n",
    "        * transition_proba is the transition probability matrix\n",
    "            [a_ijk] a_i(j*N+k) = Pr(Y_(t+1)=q_i|Y_(t-1)=q_j and Y_(t)=q_k\n",
    "        * observation_proba is the observation probablility matrix\n",
    "            [b_ki] b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "        * initial_state_proba is the initial state distribution\n",
    "            [p_ij] p_i*N+j = Pr(Y_0=q_i and Y_1=q_j)\n",
    "        * attention: We use inverse row and col rather than example in book\n",
    "        because `/` represent divide by rows in numpy.\"\"\"\n",
    "        print(\"HMM_2 creating with: \")\n",
    "        self.N = len(state_list)       # number of states\n",
    "        self.M = len(observation_list)  # number of possible emissions\n",
    "        print(str(self.N) + \" states\")\n",
    "        print(str(self.M) + \" observations\")\n",
    "        self.omega_Y = [s for s in state_list]\n",
    "        self.omega_X = [o for o in observation_list]\n",
    "        if transition_proba is None:\n",
    "            self.transition_proba = zeros((self.N, self.N**2), dtype = np.float)\n",
    "        else:\n",
    "            self.transition_proba = transition_proba\n",
    "        if observation_proba is None:\n",
    "            self.observation_proba = zeros((self.M, self.N**2), dtype = np.float)\n",
    "        else:\n",
    "            self.observation_proba = observation_proba\n",
    "        if initial_state_proba is None:\n",
    "            self.initial_state_proba = zeros(self.N**2, dtype = np.float)\n",
    "        else:\n",
    "            self.initial_state_proba = initial_state_proba\n",
    "        self.make_indexes()  # build indexes, i.e the mapping between token and int\n",
    "        self.smoothing = smoothing\n",
    "    def make_indexes(self):\n",
    "        \"\"\"Creates the reverse table that maps states/observations names\n",
    "        to their index in the probabilities array\"\"\"\n",
    "        self.Y_index = {}\n",
    "        for i in range(self.N):\n",
    "            self.Y_index[self.omega_Y[i]] = i\n",
    "        self.X_index = {}\n",
    "        for i in range(self.M):\n",
    "            self.X_index[self.omega_X[i]] = i\n",
    "        print(\"Y_index:\\n\", self.Y_index)\n",
    "        print(\"X_index:\\n\", self.X_index)\n",
    "        \n",
    "    def get_observationIndices(self, observations):\n",
    "        \"\"\"return observation indices, i.e\n",
    "        return [self.O_index[o] for o in observations]\n",
    "        and deals with OOVs\n",
    "        \"\"\"\n",
    "        indices = zeros(len(observations), int)\n",
    "        k = 0\n",
    "        for o in observations:\n",
    "            if o in self.X_index:\n",
    "                indices[k] = self.X_index[o]\n",
    "            else:\n",
    "                indices[k] = UNKid\n",
    "            k += 1\n",
    "        return indices\n",
    "    def data2indices(self, word):\n",
    "        \"\"\"From one tagged word of the given corpus:\n",
    "        - extract the letters and tags\n",
    "        - returns two list of indices, one for each\n",
    "        -> (letterids, tagids)\n",
    "        \"\"\"\n",
    "        letterids = list()\n",
    "        tagids = list()\n",
    "        for couple in word:\n",
    "            letter = couple[0]\n",
    "            tag = couple[1]\n",
    "            if letter in self.X_index:\n",
    "                letterids.append(self.X_index[letter])\n",
    "            else:\n",
    "                letterids.append(UNKid)\n",
    "            tagids.append(self.Y_index[tag])\n",
    "        return letterids, tagids\n",
    "    def observation_estimation(self, pair_counts):\n",
    "        \"\"\" Build the observation distribution:\n",
    "            observation_proba is the observation probablility matrix\n",
    "            b[k,i*N+j] = Pr(X_t=v_k|Y_t=q_j and Y_(t-1)=q_i)\"\"\"\n",
    "        # fill with counts\n",
    "        for pair in pair_counts:\n",
    "            letter = pair[1]\n",
    "            tag = pair[0][1]\n",
    "            pre_tag = pair[0][0]\n",
    "            cpt = pair_counts[pair]\n",
    "            k = 0  # for <unk>\n",
    "            if letter in self.X_index:\n",
    "                k = self.X_index[letter]\n",
    "            i = self.Y_index[pre_tag]\n",
    "            j = self.Y_index[tag]\n",
    "            self.observation_proba[k, i * self.N + j] = cpt\n",
    "        # normalize, smoothing est pour eviter prob(x(t), s(i)) == 0.\n",
    "        # Apres, on devra faire une normalisation quand meme.\n",
    "        self.observation_proba = self.observation_proba + self.smoothing\n",
    "        # smoothing > 0 et apartient Real. il n'est pas obgalitoire de etre\n",
    "        # inferieur que 1.\n",
    "        self.observation_proba = self.observation_proba / \\\n",
    "            self.observation_proba.sum(axis=0).reshape(1, self.N**2)\n",
    "    def transition_estimation(self, trans_counts):\n",
    "        \"\"\" Build the transition distribution:\n",
    "            transition_proba is the transition matrix with :\n",
    "            a[k,(i*N+j)] = Pr(Y_(t)=q_k|Y_(t-1)=q_j and Y_(t-2)=q_i)\n",
    "        \"\"\"\n",
    "        # fill with counts\n",
    "        for pair in trans_counts:\n",
    "            k = self.Y_index[pair[1]]\n",
    "            #((q_i, q_j), q_k) <==> ((Y_(t-2), Y_(t-1)), Y_(t))\n",
    "            i = self.Y_index[pair[0][0]]\n",
    "            j = self.Y_index[pair[0][1]]\n",
    "            self.transition_proba[k, (i * self.N + j)] = trans_counts[pair]\n",
    "        # normalize\n",
    "        self.transition_proba = self.transition_proba + self.smoothing\n",
    "        self.transition_proba = self.transition_proba / \\\n",
    "            self.transition_proba.sum(axis=0).reshape(1, self.N**2)\n",
    "    def init_estimation(self, init_counts):\n",
    "        \"\"\"Build the init. distribution\n",
    "        The initial distribution μ of (Y0,Y1) on S×S, such that,\n",
    "        for every states (x,y) in S×S, one has P(Y0=x,Y1=y)=μ(x,y)\"\"\"\n",
    "        # fill with counts\n",
    "        for tag in init_counts:\n",
    "            i = self.Y_index[tag[0]]\n",
    "            j = self.Y_index[tag[1]]\n",
    "            self.initial_state_proba[i * self.N + j] = init_counts[tag]\n",
    "        # normalize\n",
    "        self.initial_state_proba = self.initial_state_proba / \\\n",
    "            sum(self.initial_state_proba)\n",
    "    def supervised_training(self, pair_counts, trans_counts, init_counts):\n",
    "        \"\"\" Train the HMM_2's parameters. This function wraps everything\"\"\"\n",
    "        self.observation_estimation(pair_counts)\n",
    "        self.transition_estimation(trans_counts)\n",
    "        self.init_estimation(init_counts)\n",
    "    def viterbi(self, obs):\n",
    "        \"\"\"Viterbi algorithm:\n",
    "        Find the states corresponding to the oberservations.\n",
    "        The oberservations must be converted in a list of indices.\"\"\"\n",
    "        if len(obs)<2:\n",
    "            # the length of observation is smaller strictly than 2\n",
    "            return []\n",
    "        # shortcuts about this class's functions\n",
    "        B = self.observation_proba\n",
    "        A = self.transition_proba\n",
    "        T = len(obs)\n",
    "        N = self.N\n",
    "        # initialisation\n",
    "        # init\n",
    "        delta = zeros(N**2, float)\n",
    "        tmp = zeros(N**2, float)\n",
    "        psi = zeros((T, N**2), int)\n",
    "        delta_t = zeros(N**2, float)\n",
    "        # apply initial_state probs to the first frame\n",
    "        delta = B[obs[1]] * self.initial_state_proba\n",
    "        # recursion\n",
    "        for t in range(2, T):\n",
    "            O_t = obs[t]\n",
    "            for j in range(N):\n",
    "                for k in range(N):\n",
    "                    # tmp[0:N] represent 0<=i<=N-1\n",
    "                    tmp = delta[j::N] * A[k, j::N] # one by one multiply\n",
    "                    id_i = psi[t, j*N+k] = tmp.argmax()\n",
    "                    delta_t[j*N+k] = tmp[id_i] * B[O_t, j*N+k]\n",
    "            delta, delta_t = delta_t, delta\n",
    "        # reconstruction\n",
    "        # find j, k s.t. delta[j*N+k] is the maximal value(delta.max()).\n",
    "        j_quotient, k_reminder = divmod(delta.argmax(), N)\n",
    "        # construct the list of tags by matrix psi\n",
    "        i_star = [j_quotient, k_reminder]\n",
    "        for psi_t in psi[-1:1:-1]:\n",
    "            i_star[:0] = [psi_t[i_star[0] * N + i_star[1]]]\n",
    "        return i_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Compute what Hmm needs\n",
    "- The state transition probability distribution\n",
    "- The obervation symbol probability distribution\n",
    "- The initial state distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def make_counts(corpus):\n",
    "    \"\"\"\n",
    "    Build different count tables to train a HMM_2. Each count table is a dictionnary.\n",
    "    Returns:\n",
    "    * c_letters: letters counts\n",
    "    * c_tags: tag counts\n",
    "    * c_pairs: count of pairs (pre_tag, tag, letter)\n",
    "    * c_transitions: count of tag 3-gram\n",
    "    * c_inits: count of 2-gram found in the first and second position\n",
    "    \"\"\"\n",
    "    c_letters = dict()\n",
    "    c_tags = dict()\n",
    "    c_pairs = dict()\n",
    "    c_transitions = dict()\n",
    "    c_inits = dict()\n",
    "    for word in corpus:\n",
    "        # we use i because of the transition counts\n",
    "        for i in range(len(word)):\n",
    "            couple = word[i]\n",
    "            letter = couple[0]\n",
    "            tag = couple[1]\n",
    "            # word counts\n",
    "            if letter in c_letters:\n",
    "                c_letters[letter] = c_letters[letter] + 1\n",
    "            else:\n",
    "                c_letters[letter] = 1\n",
    "            # tag counts\n",
    "            if tag in c_tags:\n",
    "                c_tags[tag] = c_tags[tag] + 1\n",
    "            else:\n",
    "                c_tags[tag] = 1\n",
    "            if i >= 2:\n",
    "                # transition counts, z is combination of two previous states\n",
    "                z = (word[i - 2][1], word[i - 1][1])\n",
    "                trans = (z, tag)\n",
    "                if trans in c_transitions:\n",
    "                    c_transitions[trans] = c_transitions[trans] + 1\n",
    "                else:\n",
    "                    c_transitions[trans] = 1\n",
    "                # observation counts\n",
    "                o = ((word[i-1][1], tag), letter)\n",
    "                if o in c_pairs:\n",
    "                    c_pairs[o] = c_pairs[o] + 1\n",
    "                else:\n",
    "                    c_pairs[o] = 1\n",
    "            if i == 1:\n",
    "                # init counts, i == 1 -> counts for initial states\n",
    "                z = (word[i - 1][1], tag)\n",
    "                if z in c_inits:\n",
    "                    c_inits[z] = c_inits[z] + 1\n",
    "                else:\n",
    "                    c_inits[z] = 1\n",
    "                # observation counts\n",
    "                o = ((word[i - 1][1], tag), letter)\n",
    "                if o in c_pairs:\n",
    "                    c_pairs[o] = c_pairs[o] + 1\n",
    "                else:\n",
    "                    c_pairs[o] = 1\n",
    "            # i == 0 -> To present the bound, we can insert special symbol if we want\n",
    "            if i == 0:\n",
    "                continue\n",
    "    return c_letters, c_tags, c_pairs, c_transitions, c_inits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Creation of vocabulary according to the number of occurence for each letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def make_vocab(c_letters, threshold):\n",
    "    \"\"\"\n",
    "    return a vocabulary by thresholding letter counts.\n",
    "    inputs:\n",
    "    * c_letters : a dictionnary that maps letter to its counts\n",
    "    * threshold: count must be >= to the threshold to be included\n",
    "    returns:\n",
    "    * a letter list\n",
    "    \"\"\"\n",
    "    voc = list()\n",
    "    voc.append(UNK)\n",
    "    for l in c_letters:\n",
    "        if c_letters[l] >= threshold:\n",
    "            voc.append(l)\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Loading two data sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('o', 'o'), ('f', 'f')], [('p', 'l'), ('j', 'i'), ('b', 'b'), ('e', 'e'), ('r', 'r'), ('a', 'a'), ('t', 't'), ('i', 'i'), ('o', 'o'), ('n', 'n')], [('i', 'i'), ('n', 'n')]]\n"
     ]
    }
   ],
   "source": [
    "# read train data\n",
    "f_in = open(\"./data/train10.pkl\", \"rb\")\n",
    "data_train10 = pickle.load(f_in)\n",
    "f_in.close()\n",
    "# print len(data_train10)\n",
    "# read test data\n",
    "f_in = open(\"./data/test10.pkl\", \"rb\")\n",
    "data_test10 = pickle.load(f_in)\n",
    "f_in.close()\n",
    "print(data_train10[10:13])\n",
    "cletters, ctags, cpairs, ctrans, cinits = make_counts(data_train10)\n",
    "# print \"Nombre de letters  : \" + str(len(cletters))\n",
    "# print \"Nombre de tags  : \" + str(len(ctags))\n",
    "# print \"Nombre de paires: \" + str(len(cpairs))\n",
    "# print \"Nombre de trans : \" + str(len(ctrans)) + \" / \" + str(26 * 26 * 26)\n",
    "# print \"Nombre de init. : \" + str(len(cinits))\n",
    "\n",
    "vocab = make_vocab(cletters, 1)\n",
    "vocab.sort()\n",
    "# print \"----------%d vocabulaire----------\" % len(vocab)\n",
    "# print vocab\n",
    "\n",
    "states = list(ctags.keys())\n",
    "states.sort()\n",
    "# print \"----------%d states----------\" % len(states)\n",
    "# print states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Creation of HMM_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM_2 creating with: \n",
      "26 states\n",
      "27 observations\n",
      "Y_index:\n",
      " {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25}\n",
      "X_index:\n",
      " {'<unk>': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM_2(state_list=states, observation_list=vocab,\n",
    "            transition_proba=None,\n",
    "            observation_proba=None,\n",
    "            initial_state_proba=None)\n",
    "hmm.supervised_training(cpairs, ctrans, cinits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Calculating three neccessary probability distributions one by one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "hmm.observation_estimation(cpairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "hmm.transition_estimation(ctrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "hmm.init_estimation(cinits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Training data one time(we can get three neccessary matrix once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy(%) : 6292.0 / 7320.0 -> 85.95628415300547\n"
     ]
    }
   ],
   "source": [
    "# hmm.supervised_training(cpairs, ctrans, cinits)\n",
    "tot=0.0\n",
    "correct=0.0\n",
    "for word in data_test10:\n",
    "    letter_index, tag_index = hmm.data2indices(word)\n",
    "    pre_tag_index = hmm.viterbi(letter_index)\n",
    "    correct += np.count_nonzero(np.array(tag_index) == np.array(pre_tag_index))\n",
    "    tot+=len(word)\n",
    "print(\"The accuracy(%) : \"+str(correct)+\" / \"+str(tot)+ \" -> \"+ str(correct*100/tot))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "name": "TC4-tp2-correction-1.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
